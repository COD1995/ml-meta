<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Deep Learning Crash Course</title>

    <!-- Global styles -->
    <link rel="stylesheet" href="../../assets/css/base.css" />
    <link rel="stylesheet" href="../../../assets/css/inline-styles.css" />
    <link rel="stylesheet" href="../../assets/css/chapters.css" />

    <!-- Library styles -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css"
    />

    <!-- MathJax global config (must come BEFORE MathJax) -->
    <script src="../../assets/js/mathjax-config.js"></script>

    <!-- Library scripts (deferred) -->
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"
    ></script>
    <script
      defer
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.8.0/dist/highlightjs-line-numbers.min.js"
    ></script>
  </head>

  <body>
    <!-- Reading Progress Indicator -->
    <div id="reading-progress">
      <div id="reading-progress-bar"></div>
    </div>

    <nav class="side-nav">
      <div class="side-nav-controls">
        <button
          id="homeBtn"
          class="btn-toggle"
          aria-label="Home"
          onclick="location.href='../../index.html'"
        >
          üè†
        </button>
        <span class="nav-divider"></span>
        <button
          id="themeToggle"
          class="btn-toggle"
          aria-label="Toggle dark mode"
        >
          üåì
        </button>
      </div>

      <hr class="side-nav-divider" />

      <!-- everything below is built by build-side-nav.js -->
      <div id="bookNav"></div>

      <hr class="side-nav-divider" />

      <!-- Buy Me a Coffee Banner -->
      <div class="coffee-banner">
        <a
          href="https://buymeacoffee.com/guoj1995"
          id="coffeeButton"
          target="_blank"
          rel="noopener noreferrer"
          aria-label="Support ML-Meta on Buy Me a Coffee"
        >
          <svg
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
          >
            <path d="M20 3H4V5H20V3Z" fill="currentColor" />
            <path
              d="M20 7H4C2.9 7 2 7.9 2 9V17C2 18.1 2.9 19 4 19H10C11.1 19 12 18.1 12 17V16H16C18.2 16 20 14.2 20 12V9C20 7.9 19.1 7 18 7H20Z"
              fill="currentColor"
            />
          </svg>
          <span>Support Us</span>
        </a>
      </div>

      <!-- Slack banner (unchanged) -->
      <div class="slack-banner">
        <a
          href="https://join.slack.com/t/mlmetacommunity/shared_invite/zt-38mj0hx5v-8GyxvZ7lanC9HbywfUOwJw"
          id="slackButton"
          target="_blank"
          rel="noopener"
          aria-label="Join our Slack Community"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            viewBox="0 0 122.8 122.8"
            fill="#fff"
            style="width: 1.5em; height: 1.5em; vertical-align: middle"
          >
            <path d="M30.3 78.6c0 5-4 9-9 9s-9-4-9-9 4-9 9-9h9v9z" />
            <path
              d="M34.8 78.6c0-5 4-9 9-9s9 4 9 9v22.5c0 5-4 9-9 9s-9-4-9-9V78.6z"
            />
            <path d="M44 30.3c-5 0-9-4-9-9s4-9 9-9 9 4 9 9v9H44z" />
            <path
              d="M44 34.8c5 0 9 4 9 9s-4 9-9 9H21.5c-5 0-9-4-9-9s4-9 9-9H44z"
            />
            <path d="M92.5 44c0-5 4-9 9-9s9 4 9 9-4 9-9 9h-9V44z" />
            <path
              d="M88 44c0 5-4 9-9 9s-9-4-9-9V21.5c0-5 4-9 9-9s9 4 9 9V44z"
            />
            <path d="M78.8 92.5c5 0 9 4 9 9s-4 9-9 9-9-4-9-9v-9h9z" />
            <path
              d="M78.8 88c-5 0-9-4-9-9s4-9 9-9h22.5c5 0 9 4 9 9s-4 9-9 9H78.8z"
            />
          </svg>
          <span style="margin-left: 0.5em; vertical-align: middle"
            >Join our Slack</span
          >
        </a>
      </div>
    </nav>

    <div class="container content">
      <h1>Deep Learning Crash Course</h1>

      <section id="what-is-deep-learning">
        <blockquote>
          Deep learning is a subset of machine learning that uses artificial
          neural networks with multiple layers to automatically learn patterns
          from data. Unlike traditional programming where we write explicit
          rules, deep learning systems discover these patterns through examples.
        </blockquote>

        <p>
          Think of deep learning as teaching a computer to recognize patterns
          the same way a child learns - through repeated exposure to examples.
          The "deep" refers to the multiple layers of artificial neurons that
          process information in increasingly complex ways, much like how our
          brains process visual information from simple edges to complex
          objects.
        </p>
      </section>

      <section id="neural-networks-fundamentals">
        <h2>1. Neural Network Fundamentals</h2>

        <blockquote>
          An artificial neuron is a mathematical function that takes multiple
          inputs, applies weights to them, adds a bias term, and passes the
          result through an activation function to produce an output.
        </blockquote>

        <p>
          The basic neuron operation can be expressed as: $$y = f(w_1x_1 +
          w_2x_2 + ... + w_nx_n + b)$$ where $f$ is the activation function,
          $w_i$ are weights, $x_i$ are inputs, and $b$ is the bias term. This
          simple mathematical operation becomes powerful when combined with
          thousands or millions of other neurons.
        </p>

        <blockquote>
          A neural network consists of three types of layers: an input layer
          that receives raw data, one or more hidden layers that process the
          information, and an output layer that produces the final prediction or
          classification.
        </blockquote>

        <p>
          The architecture matters significantly. Deeper networks (with more
          hidden layers) can learn more complex patterns, but they also require
          more data and computational power to train effectively. The key
          insight is that each layer learns to detect increasingly sophisticated
          features - from simple patterns in early layers to complex concepts in
          deeper layers.
        </p>
      </section>

      <section id="training-process">
        <h2>2. How Neural Networks Learn</h2>

        <blockquote>
          Training a neural network involves showing it many examples of
          input-output pairs and gradually adjusting the weights and biases to
          minimize the difference between predicted and actual outputs. This
          process uses an algorithm called backpropagation.
        </blockquote>

        <p>The training process follows these steps:</p>
        <ol>
          <li>
            <strong>Forward Pass:</strong> Data flows through the network to
            make predictions
          </li>
          <li>
            <strong>Loss Calculation:</strong> Compare predictions with true
            answers using a loss function
          </li>
          <li>
            <strong>Backward Pass:</strong> Calculate gradients showing how to
            adjust each weight
          </li>
          <li>
            <strong>Parameter Update:</strong> Adjust weights in the direction
            that reduces error
          </li>
          <li>
            <strong>Repeat:</strong> Continue this process until the network
            performs well
          </li>
        </ol>

        <blockquote>
          The learning rate controls how big steps we take when updating
          weights. Too large, and the network might overshoot the optimal
          solution; too small, and training becomes prohibitively slow.
        </blockquote>

        <pre class="pseudocode" id="training-algorithm">
          \begin{algorithm}
          \caption{Neural Network Training}
          \begin{algorithmic}
            \For{each epoch}
              \For{each batch of data}
                \State predictions $\gets$ forward\_pass(input\_data)
                \State loss $\gets$ loss\_function(predictions, true\_labels)
                \State gradients $\gets$ backward\_pass(loss)
                \State weights $\gets$ weights - learning\_rate $\times$ gradients
              \EndFor
            \EndFor
          \end{algorithmic}
          \end{algorithm}
        </pre>

        <p>
          This algorithm is deceptively simple but incredibly powerful. The
          magic happens in the backward pass, where calculus (specifically the
          chain rule) is used to determine exactly how each weight contributed
          to the final error, allowing precise adjustments across the entire
          network.
        </p>
      </section>

      <section id="backpropagation">
        <h3>2.1. Backpropagation Explained</h3>

        <blockquote>
          Backpropagation is the algorithm that computes gradients for each
          weight in the network by applying the chain rule of calculus. It
          allows us to efficiently calculate how much each weight contributed to
          the overall error.
        </blockquote>

        <p>
          The key idea, introduced by Geoffrey Hinton, is to propagate the error
          backward through the network, layer by layer, adjusting weights based
          on their contribution to the error. This process enables deep networks
          to learn complex mappings from inputs to outputs.
        </p>

        <figure class="nn-diagram" style="margin: 2rem 0">
          <img
            src="../../assets/svg/papers/basics/deep-learning-crash-course/neural-network-basic-structure.svg"
            alt="Two-input, two-hidden-neuron, one-output network with weights w1 to w6"
            width="100%"
          />
        </figure>

        <p>
          In this diagram, a representation of a simple neural network, the
          arrows represent the flow of information through the network. Each
          weight (w1 to w6) adjusts how much influence each input has on the
          hidden neurons, and how those hidden neurons affect the final output.
          The backpropagation algorithm calculates how to adjust these weights
          based on the error in the output.
        </p>
        <p>
          <strong>Always about weights</strong> Neural networks learn by
          adjusting the weights of connections between neurons. The training
          process involves finding the optimal weights that minimize the error
          between predicted and actual outputs. This is done through iterative
          updates using the gradients calculated during backpropagation. But you
          need some initial values for the weights, which are typically set
          randomly at the start of training.
        </p>

        <p>
          In our case, we initiate the weights as the following
          <code>w1 = 0.11</code>, <code>w2 = 0.21</code>,
          <code>w3 = 0.12</code>, <code>w4 = 0.08</code>,
          <code>w5 = 0.14</code>, and <code>w6 = 0.15</code>.
        </p>

        <figure class="nn-diagram" style="margin: 2rem 0">
          <img
            src="../../assets/svg/papers/basics/deep-learning-crash-course/neural-network-with-weights.svg"
            alt="Two-input, two-hidden-neuron, one-output network with weights w1 to w6"
            width="100%"
          />
        </figure>

        <p>
          <strong>Dataset</strong> We have our neural network with initial
          weights. I want it to learn something, how does neural network learn?
          Data !!! For demonstration, let's say we have a dataset with two
          inputs and one output. The dataset is as follows:
          <figure style="margin: 1rem 0">
            <svg
              viewBox="0 0 300 100"
              width="60%"
              role="img"
              aria-label="Two input values and actual output"
            >
              <style>
                .label {
                  font: 600 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #777;
                }
                .circle-text {
                  font: 700 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .divider {
                  stroke: #ccc;
                  stroke-dasharray: 4 4;
                  stroke-width: 1.5;
                }
              </style>

              <!-- Labels -->
              <text class="label" x="60" y="22" text-anchor="middle">
                Input
              </text>
              <text class="label" x="220" y="22" text-anchor="middle">
                Actual output
              </text>

              <!-- Input circles (extra spacing, still centered) -->
              <circle cx="35" cy="60" r="20" fill="#ffb400" />
              <text class="circle-text" x="35" y="60">2</text>

              <circle cx="85" cy="60" r="20" fill="#ffb400" />
              <text class="circle-text" x="85" y="60">3</text>

              <!-- Divider -->
              <line x1="130" y1="15" x2="130" y2="85" class="divider" />

              <!-- Actual output circle -->
              <circle cx="220" cy="60" r="20" fill="#7b3fc1" />
              <text class="circle-text" x="220" y="60">1</text>
            </svg>
          </figure>
        </p>

        <strong>Forward Pass</strong>
        <!-- Forward pass network diagram: shows how the initial inputs (2 and 3) 
     flow through the network with the current weights, activations, and forward-pass arrow -->
        <figure class="nn-diagram" style="margin: 2rem 0">
          <img
            src="../../assets/svg/papers/basics/deep-learning-crash-course/neural-network-forward-pass.svg"
            alt="Forward pass of two-input, two-hidden-neuron, one-output network with numeric activations and weights"
            width="100%"
          />
        </figure>

        <p>
          During the forward pass, the network makes predictions based on the
          current weights. The output is compared to the actual target, and the
          difference (loss) is computed. How is it calculated? $$
          \left[\begin{array}{ll}2 & 3\end{array}\right]
          \cdot\left[\begin{array}{ll}0.11 & 0.12 \\ 0.21 &
          0.08\end{array}\right]=\left[\begin{array}{ll}0.85 &
          0.48\end{array}\right] \cdot\left[\begin{array}{l}0.14 \\
          0.15\end{array}\right]=[0.191] $$ For those of you who are not
          familiar with matrix multiplication, here's a quick breakdown: $$
          \begin{array}{ll}2 \times .11+3 \times .21=.85 & .85 \times .14+.48
          \times .15=.191 \\ 2 \times .12+3 \times .08=.48 & \end{array} $$
        </p>

        <p>
          <strong>Calculate Loss</strong> The first prediction is not ideal, it
          is actually very far off. So what can we do about it? We need to
          calculate the loss. The loss function quantifies how far off the
          predictions are from the actual targets. A common choice for
          regression tasks is the Mean Squared Error (MSE) loss, defined as: $$
          \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$ where
          $y_i$ is the actual target and $\hat{y}_i$ is the predicted value.
          Now, $$ \text { Error }=\frac{1}{2}(0.191-1.0)^{2}=0.327 $$
        </p>

        <p>
          <strong>Reducing Error</strong> Our goal is to reduce this error
          <em
            >(Since the actual output is constant, the only way to reduce the
            error is to change the prediction value). One way to do this is by
            adjusting</em
          >
          the weights in the network. This is where the magic of backpropagation
          comes in. By computing the gradient of the loss with respect to each
          weight, we can determine how to change the weights to minimize the
          loss. This process is repeated over many iterations (epochs) until the
          network learns to make accurate predictions. $$ \require{color}
          \begin{aligned} \mathbf{prediction} &=
          \textcolor[RGB]{102,187,106}{\mathbf{out}} \\[4pt] \mathbf{prediction}
          &=
          \big(\textcolor[RGB]{3,155,229}{\mathbf{h_1}}\big)\,\textcolor[RGB]{138,138,138}{\mathbf{w_5}}
          +
          \big(\textcolor[RGB]{3,155,229}{\mathbf{h_2}}\big)\,\textcolor[RGB]{138,138,138}{\mathbf{w_6}}
          \\[4pt] \mathbf{prediction} &=
          \big(i_{1}\,\textcolor[RGB]{60,100,177}{\mathbf{w_1}} +
          i_{2}\,\textcolor[RGB]{60,100,177}{\mathbf{w_2}}\big)\,\textcolor[RGB]{138,138,138}{\mathbf{w_5}}
          + \big(i_{1}\,\textcolor[RGB]{240,123,63}{\mathbf{w_3}} +
          i_{2}\,\textcolor[RGB]{240,123,63}{\mathbf{w_4}}\big)\,\textcolor[RGB]{138,138,138}{\mathbf{w_6}}
          \end{aligned} $$
        </p>

        <p>
          <strong>Backpropagation</strong> The vanilla version of it is gradient
          descent.Gradient descent is an iterative optimization algorithm for
          finding the minimum of a function; in our case we want to minimize th
          error function. To find a local minimum of a function using gradient
          descent, one takes steps proportional to the negative of the gradient
          of the function at the current point.
        </p>

        <!-- Gradient-descent weight update with annotated arrows -->
        <figure class="eq-update" style="margin: 2rem 0; text-align: center">
          <style>
            .eq-wrap {
              position: relative;
              display: inline-block;
              width: 680px;
            }
            .eq-update .label {
              position: absolute;
              color: #8c8c8c;
              font: 18px/1.2 "Segoe UI", system-ui, sans-serif;
              text-align: center;
              white-space: nowrap;
            }
            /* arrows pointing DOWN from the label */
            .eq-update .label.top::after {
              content: "";
              position: absolute;
              top: 100%;
              left: 50%;
              transform: translateX(-50%);
              width: 2px;
              height: 28px;
              background: #bdbdbd;
            }
            .eq-update .label.top::before {
              content: "";
              position: absolute;
              top: calc(100% + 28px);
              left: 50%;
              transform: translateX(-50%);
              border-left: 6px solid transparent;
              border-right: 6px solid transparent;
              border-top: 6px solid #bdbdbd;
            }
            /* arrows pointing UP from the label */
            .eq-update .label.bottom::after {
              content: "";
              position: absolute;
              bottom: 100%;
              left: 50%;
              transform: translateX(-50%);
              width: 2px;
              height: 28px;
              background: #bdbdbd;
            }
            .eq-update .label.bottom::before {
              content: "";
              position: absolute;
              bottom: calc(100% + 28px);
              left: 50%;
              transform: translateX(-50%);
              border-left: 6px solid transparent;
              border-right: 6px solid transparent;
              border-bottom: 6px solid #bdbdbd;
            }
          </style>

          <div class="eq-wrap">
            <!-- The equation (MathJax) -->
            $$ \require{ams}\require{color} \mathbf{W_x^{*}} \;=\; \mathbf{W_x}
            \;-\; \textcolor[RGB]{255,180,0}{\boldsymbol{a}} \left(
            \frac{\partial \mathrm{Error}}{\partial W_x} \right) $$

            <!-- Labels (positions are percentages across the container; tweak if needed) -->
            <div class="label bottom" style="left: 14%; bottom: -6px">
              New weight
            </div>
            <div class="label top" style="left: 39%; top: -8px">Old weight</div>
            <div class="label bottom" style="left: 53%; bottom: -6px">
              Learning rate
            </div>
            <div class="label top" style="left: 77%; top: -8px">
              Derivative of Error<br />with respect to weight
            </div>
          </div>
        </figure>
      </section>

      <section id="activation-functions">
        <h2>3. Activation Functions</h2>

        <blockquote>
          Activation functions introduce non-linearity into neural networks.
          Without them, no matter how many layers we stack, the network would
          only be capable of learning linear relationships - severely limiting
          its power.
        </blockquote>

        <p>Common activation functions include:</p>
        <ul>
          <li>
            <strong>ReLU (Rectified Linear Unit):</strong> $f(x) = \max(0, x)$ -
            Simple, effective, most popular
          </li>
          <li>
            <strong>Sigmoid:</strong> $f(x) = \frac{1}{1 + e^{-x}}$ - Outputs
            between 0 and 1
          </li>
          <li>
            <strong>Tanh:</strong> $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ -
            Outputs between -1 and 1
          </li>
          <li>
            <strong>Softmax:</strong> Converts outputs to probabilities for
            multi-class classification
          </li>
        </ul>
        <p>
          ReLU's popularity stems from its simplicity and effectiveness at
          mitigating the vanishing gradient problem that plagued earlier
          activation functions.
        </p>
      </section>

      <section id="comments">
        <script
          src="https://utteranc.es/client.js"
          repo="COD1995/ml-meta"
          issue-term="pathname"
          label="comment"
          theme="github-light"
          crossorigin="anonymous"
          async
        ></script>
      </section>

      <footer>
        <p>
          ¬© 2025 Bob Guo ‚Ä¢
          <a
            href="https://github.com/COD1995/ml-meta"
            target="_blank"
            rel="noopener"
          >
            View on GitHub
          </a>
        </p>
      </footer>
    </div>
    <script type="module" src="../../assets/js/main.js"></script>
    <script type="module" src="../../assets/js/chapter-page.js"></script>

    <!-- 1Ô∏è‚É£ auto‚Äëgenerated by the build script (local to this folder) -->
    <script src="book-data.js"></script>

    <!-- 2Ô∏è‚É£ builds the sidebar & mini‚ÄëTOC in the browser -->
    <script type="module" src="../../assets/js/build-side-nav.js"></script>
  </body>
</html>
