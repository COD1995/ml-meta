<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Deep Learning Crash Course</title>

    <!-- Global styles -->
    <link rel="stylesheet" href="../../assets/css/base.css" />
    <link rel="stylesheet" href="../../../assets/css/inline-styles.css" />
    <link rel="stylesheet" href="../../assets/css/chapters.css" />

    <!-- Library styles -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css"
    />

    <!-- MathJax global config (must come BEFORE MathJax) -->
    <script src="../../assets/js/mathjax-config.js"></script>

    <!-- Library scripts (deferred) -->
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"
    ></script>
    <script
      defer
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.8.0/dist/highlightjs-line-numbers.min.js"
    ></script>
  </head>

  <body>
    <!-- Reading Progress Indicator -->
    <div id="reading-progress">
      <div id="reading-progress-bar"></div>
    </div>

    <nav class="side-nav">
      <div class="side-nav-controls">
        <button
          id="homeBtn"
          class="btn-toggle"
          aria-label="Home"
          onclick="location.href='../../index.html'"
        >
          üè†
        </button>
        <span class="nav-divider"></span>
        <button
          id="themeToggle"
          class="btn-toggle"
          aria-label="Toggle dark mode"
        >
          üåì
        </button>
      </div>

      <hr class="side-nav-divider" />

      <!-- everything below is built by build-side-nav.js -->
      <div id="bookNav"></div>

      <hr class="side-nav-divider" />

      <!-- Buy Me a Coffee Banner -->
      <div class="coffee-banner">
        <a
          href="https://buymeacoffee.com/guoj1995"
          id="coffeeButton"
          target="_blank"
          rel="noopener noreferrer"
          aria-label="Support ML-Meta on Buy Me a Coffee"
        >
          <svg
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
          >
            <path d="M20 3H4V5H20V3Z" fill="currentColor" />
            <path
              d="M20 7H4C2.9 7 2 7.9 2 9V17C2 18.1 2.9 19 4 19H10C11.1 19 12 18.1 12 17V16H16C18.2 16 20 14.2 20 12V9C20 7.9 19.1 7 18 7H20Z"
              fill="currentColor"
            />
          </svg>
          <span>Support Us</span>
        </a>
      </div>

      <!-- Slack banner (unchanged) -->
      <div class="slack-banner">
        <a
          href="https://join.slack.com/t/mlmetacommunity/shared_invite/zt-38mj0hx5v-8GyxvZ7lanC9HbywfUOwJw"
          id="slackButton"
          target="_blank"
          rel="noopener"
          aria-label="Join our Slack Community"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            viewBox="0 0 122.8 122.8"
            fill="#fff"
            style="width: 1.5em; height: 1.5em; vertical-align: middle"
          >
            <path d="M30.3 78.6c0 5-4 9-9 9s-9-4-9-9 4-9 9-9h9v9z" />
            <path
              d="M34.8 78.6c0-5 4-9 9-9s9 4 9 9v22.5c0 5-4 9-9 9s-9-4-9-9V78.6z"
            />
            <path d="M44 30.3c-5 0-9-4-9-9s4-9 9-9 9 4 9 9v9H44z" />
            <path
              d="M44 34.8c5 0 9 4 9 9s-4 9-9 9H21.5c-5 0-9-4-9-9s4-9 9-9H44z"
            />
            <path d="M92.5 44c0-5 4-9 9-9s9 4 9 9-4 9-9 9h-9V44z" />
            <path
              d="M88 44c0 5-4 9-9 9s-9-4-9-9V21.5c0-5 4-9 9-9s9 4 9 9V44z"
            />
            <path d="M78.8 92.5c5 0 9 4 9 9s-4 9-9 9-9-4-9-9v-9h9z" />
            <path
              d="M78.8 88c-5 0-9-4-9-9s4-9 9-9h22.5c5 0 9 4 9 9s-4 9-9 9H78.8z"
            />
          </svg>
          <span style="margin-left: 0.5em; vertical-align: middle"
            >Join our Slack</span
          >
        </a>
      </div>
    </nav>

    <div class="container content">
      <h1>Deep Learning Crash Course</h1>

      <section id="what-is-deep-learning">
        <blockquote>
          Deep learning is a subset of machine learning that uses artificial
          neural networks with multiple layers to automatically learn patterns
          from data. Unlike traditional programming where we write explicit
          rules, deep learning systems discover these patterns through examples.
        </blockquote>

        <p>
          Think of deep learning as teaching a computer to recognize patterns
          the same way a child learns - through repeated exposure to examples.
          The "deep" refers to the multiple layers of artificial neurons that
          process information in increasingly complex ways, much like how our
          brains process visual information from simple edges to complex
          objects.
        </p>
      </section>

      <section id="neural-networks-fundamentals">
        <h2>1. Neural Network Fundamentals</h2>

        <blockquote>
          An artificial neuron is a mathematical function that takes multiple
          inputs, applies weights to them, adds a bias term, and passes the
          result through an activation function to produce an output.
        </blockquote>

        <p>
          The basic neuron operation can be expressed as: $$y = f(w_1x_1 +
          w_2x_2 + ... + w_nx_n + b)$$ where $f$ is the activation function,
          $w_i$ are weights, $x_i$ are inputs, and $b$ is the bias term. This
          simple mathematical operation becomes powerful when combined with
          thousands or millions of other neurons.
        </p>

        <blockquote>
          A neural network consists of three types of layers: an input layer
          that receives raw data, one or more hidden layers that process the
          information, and an output layer that produces the final prediction or
          classification.
        </blockquote>

        <p>
          The architecture matters significantly. Deeper networks (with more
          hidden layers) can learn more complex patterns, but they also require
          more data and computational power to train effectively. The key
          insight is that each layer learns to detect increasingly sophisticated
          features - from simple patterns in early layers to complex concepts in
          deeper layers.
        </p>
      </section>

      <section id="training-process">
        <h2>2. How Neural Networks Learn</h2>

        <blockquote>
          Training a neural network involves showing it many examples of
          input-output pairs and gradually adjusting the weights and biases to
          minimize the difference between predicted and actual outputs. This
          process uses an algorithm called backpropagation.
        </blockquote>

        <p>The training process follows these steps:</p>
        <ol>
          <li>
            <strong>Forward Pass:</strong> Data flows through the network to
            make predictions
          </li>
          <li>
            <strong>Loss Calculation:</strong> Compare predictions with true
            answers using a loss function
          </li>
          <li>
            <strong>Backward Pass:</strong> Calculate gradients showing how to
            adjust each weight
          </li>
          <li>
            <strong>Parameter Update:</strong> Adjust weights in the direction
            that reduces error
          </li>
          <li>
            <strong>Repeat:</strong> Continue this process until the network
            performs well
          </li>
        </ol>

        <blockquote>
          The learning rate controls how big steps we take when updating
          weights. Too large, and the network might overshoot the optimal
          solution; too small, and training becomes prohibitively slow.
        </blockquote>

        <pre class="pseudocode" id="training-algorithm">
          \begin{algorithm}
          \caption{Neural Network Training}
          \begin{algorithmic}
            \For{each epoch}
              \For{each batch of data}
                \State predictions $\gets$ forward\_pass(input\_data)
                \State loss $\gets$ loss\_function(predictions, true\_labels)
                \State gradients $\gets$ backward\_pass(loss)
                \State weights $\gets$ weights - learning\_rate $\times$ gradients
              \EndFor
            \EndFor
          \end{algorithmic}
          \end{algorithm}
        </pre>

        <p>
          This algorithm is deceptively simple but incredibly powerful. The
          magic happens in the backward pass, where calculus (specifically the
          chain rule) is used to determine exactly how each weight contributed
          to the final error, allowing precise adjustments across the entire
          network.
        </p>
      </section>

      <section id="backpropagation">
        <h3>2.1. Backpropagation Explained</h3>

        <blockquote>
          Backpropagation is the algorithm that computes gradients for each
          weight in the network by applying the chain rule of calculus. It
          allows us to efficiently calculate how much each weight contributed to
          the overall error.
        </blockquote>

        <p>
          The key idea, introduced by Geoffrey Hinton, is to propagate the error
          backward through the network, layer by layer, adjusting weights based
          on their contribution to the error. This process enables deep networks
          to learn complex mappings from inputs to outputs.
        </p>

        <figure class="nn-diagram" style="margin: 2rem 0">
          <svg
            viewBox="0 0 900 360"
            width="100%"
            role="img"
            aria-label="Two-input, two-hidden-neuron, one-output network with weights w1 to w6"
          >
            <defs>
              <marker
                id="arrow"
                markerWidth="6"
                markerHeight="5"
                refX="4.5"
                refY="2.5"
                orient="auto"
              >
                <path d="M0,0 L6,2.5 L0,5 Z" fill="#222"></path>
              </marker>

              <style>
                .layer {
                  fill: none;
                  stroke: #d9d9d9;
                  stroke-width: 2.5;
                  stroke-dasharray: 6 8;
                  rx: 18;
                  ry: 18;
                }
                .edge {
                  fill: none;
                  stroke: #222;
                  stroke-width: 2.5;
                  marker-end: url(#arrow);
                }
                .node text {
                  font: 700 18px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .node circle {
                  filter: drop-shadow(0 1.5px 2px rgba(0, 0, 0, 0.15));
                }
                .i circle {
                  fill: #ffb400;
                }
                .h circle {
                  fill: #04b5e2;
                }
                .o circle {
                  fill: #88d869;
                }
                .w rect {
                  rx: 8;
                  ry: 8;
                  filter: drop-shadow(0 1.5px 2px rgba(0, 0, 0, 0.15));
                }
                .w text {
                  font: 700 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .w12 rect {
                  fill: #3c64b1;
                }
                .w34 rect {
                  fill: #f07b3f;
                }
                .w56 rect {
                  fill: #8a8a8a;
                }
                .layer-label {
                  font: 600 22px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #6b6b6b;
                }
                .pred {
                  font: 500 18px system-ui;
                  fill: #777;
                }
              </style>
            </defs>

            <!-- Layer boxes & labels -->
            <g>
              <rect class="layer" x="30" y="40" width="220" height="280"></rect>
              <rect
                class="layer"
                x="310"
                y="40"
                width="220"
                height="280"
              ></rect>
              <rect
                class="layer"
                x="590"
                y="40"
                width="260"
                height="280"
              ></rect>

              <text class="layer-label" x="60" y="32">Input layer</text>
              <text class="layer-label" x="340" y="32">Hidden layer</text>
              <text class="layer-label" x="620" y="32">Output layer</text>
            </g>

            <!-- Edges (left -> right) -->
            <path class="edge" d="M 190 120 L 380 120"></path>
            <path class="edge" d="M 190 120 L 380 240"></path>
            <path class="edge" d="M 190 260 L 380 120"></path>
            <path class="edge" d="M 190 260 L 380 240"></path>

            <!-- hidden -> output (end just before OUT circle) -->
            <!-- draw these AFTER the nodes so arrowheads aren't hidden -->
            <!-- Adjusted endpoints to land just outside the output node radius (r=45) -->
            <path class="edge" d="M 500 150 L 714 191"></path>
            <!-- h1 -> out (adjusted) -->
            <path class="edge" d="M 500 230 L 712 203"></path>
            <!-- h2 -> out -->

            <!-- Weight pills (w4 raised more) -->
            <g class="w w12">
              <rect x="310" y="95" width="46" height="32"></rect>
              <text x="333" y="111">w1</text>
            </g>
            <g class="w w12">
              <rect x="310" y="140" width="46" height="32"></rect>
              <text x="333" y="156">w2</text>
            </g>
            <g class="w w34">
              <rect x="310" y="198" width="46" height="32"></rect>
              <text x="333" y="214">w3</text>
            </g>
            <g class="w w34">
              <rect x="310" y="237" width="46" height="32"></rect>
              <text x="333" y="253">w4</text>
            </g>
            <!-- FIXED positions (further left, w5 raised more, w6 lowered more) -->
            <g class="w w56">
              <rect x="600" y="156" width="46" height="36"></rect>
              <text x="623" y="179">w5</text>
            </g>
            <g class="w w56">
              <rect x="600" y="198" width="46" height="36"></rect>
              <text x="623" y="217">w6</text>
            </g>

            <!-- Nodes -->
            <g class="node i">
              <circle cx="120" cy="120" r="45"></circle>
              <text x="120" y="120">i‚ÇÅ</text>
            </g>
            <g class="node i">
              <circle cx="120" cy="260" r="45"></circle>
              <text x="120" y="260">i‚ÇÇ</text>
            </g>
            <g class="node h">
              <circle cx="440" cy="120" r="45"></circle>
              <text x="440" y="120">h‚ÇÅ</text>
            </g>
            <g class="node h">
              <circle cx="440" cy="240" r="45"></circle>
              <text x="440" y="240">h‚ÇÇ</text>
            </g>
            <g class="node o">
              <circle cx="760" cy="200" r="45"></circle>
              <text x="760" y="200">out</text>
            </g>

            <text class="pred" x="815" y="150" text-anchor="middle">
              prediction
            </text>
            <!-- Arrow from center-bottom of 'prediction' text to edge of output node -->
            <path
              d="M 815 158 L 791 180"
              stroke="#777"
              stroke-width="2.5"
              fill="none"
              marker-end="url(#arrow)"
            />
          </svg>
        </figure>

        <p>
          In this diagram, a representation of a simple neural network, the
          arrows represent the flow of information through the network. Each
          weight (w1 to w6) adjusts how much influence each input has on the
          hidden neurons, and how those hidden neurons affect the final output.
          The backpropagation algorithm calculates how to adjust these weights
          based on the error in the output.
        </p>
        <p>
          <strong>Always about weights</strong> Neural networks learn by
          adjusting the weights of connections between neurons. The training
          process involves finding the optimal weights that minimize the error
          between predicted and actual outputs. This is done through iterative
          updates using the gradients calculated during backpropagation. But you
          need some initial values for the weights, which are typically set
          randomly at the start of training.
        </p>

        <p>
          In our case, we initiate the weights as the following
          <code>w1 = 0.11</code>, <code>w2 = 0.21</code>,
          <code>w3 = 0.12</code>, <code>w4 = 0.08</code>,
          <code>w5 = 0.14</code>, and <code>w6 = 0.15</code>.
        </p>

        <figure class="nn-diagram" style="margin: 2rem 0">
          <svg
            viewBox="0 0 900 360"
            width="100%"
            role="img"
            aria-label="Two-input, two-hidden-neuron, one-output network with weights w1 to w6"
          >
            <defs>
              <marker
                id="arrow"
                markerWidth="6"
                markerHeight="5"
                refX="4.5"
                refY="2.5"
                orient="auto"
              >
                <path d="M0,0 L6,2.5 L0,5 Z" fill="#222"></path>
              </marker>

              <style>
                .layer {
                  fill: none;
                  stroke: #d9d9d9;
                  stroke-width: 2.5;
                  stroke-dasharray: 6 8;
                  rx: 18;
                  ry: 18;
                }
                .edge {
                  fill: none;
                  stroke: #222;
                  stroke-width: 2.5;
                  marker-end: url(#arrow);
                }
                .node text {
                  font: 700 18px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .node circle {
                  filter: drop-shadow(0 1.5px 2px rgba(0, 0, 0, 0.15));
                }
                .i circle {
                  fill: #ffb400;
                }
                .h circle {
                  fill: #04b5e2;
                }
                .o circle {
                  fill: #88d869;
                }
                .w rect {
                  rx: 8;
                  ry: 8;
                  filter: drop-shadow(0 1.5px 2px rgba(0, 0, 0, 0.15));
                }
                .w text {
                  font: 700 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .w12 rect {
                  fill: #3c64b1;
                }
                .w34 rect {
                  fill: #f07b3f;
                }
                .w56 rect {
                  fill: #8a8a8a;
                }
                .layer-label {
                  font: 600 22px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #6b6b6b;
                }
                .pred {
                  font: 500 18px system-ui;
                  fill: #777;
                }
              </style>
            </defs>

            <!-- Layer boxes & labels -->
            <g>
              <rect class="layer" x="30" y="40" width="220" height="280"></rect>
              <rect
                class="layer"
                x="310"
                y="40"
                width="220"
                height="280"
              ></rect>
              <rect
                class="layer"
                x="590"
                y="40"
                width="260"
                height="280"
              ></rect>

              <text class="layer-label" x="60" y="32">Input layer</text>
              <text class="layer-label" x="340" y="32">Hidden layer</text>
              <text class="layer-label" x="620" y="32">Output layer</text>
            </g>

            <!-- Edges (left -> right) -->
            <path class="edge" d="M 190 120 L 380 120"></path>
            <path class="edge" d="M 190 120 L 380 240"></path>
            <path class="edge" d="M 190 260 L 380 120"></path>
            <path class="edge" d="M 190 260 L 380 240"></path>

            <!-- hidden -> output (end just before OUT circle) -->
            <!-- draw these AFTER the nodes so arrowheads aren't hidden -->
            <!-- Adjusted endpoints to land just outside the output node radius (r=45) -->
            <path class="edge" d="M 500 150 L 714 191"></path>
            <!-- h1 -> out (adjusted) -->
            <path class="edge" d="M 500 230 L 712 203"></path>
            <!-- h2 -> out -->

            <!-- Weight pills with values -->
            <g class="w w12">
              <rect x="298" y="95" width="70" height="32"></rect>
              <text x="333" y="111">0.11</text>
            </g>
            <g class="w w12">
              <rect x="298" y="140" width="70" height="32"></rect>
              <text x="333" y="156">0.21</text>
            </g>
            <g class="w w34">
              <rect x="298" y="198" width="70" height="32"></rect>
              <text x="333" y="214">0.12</text>
            </g>
            <g class="w w34">
              <rect x="298" y="237" width="70" height="32"></rect>
              <text x="333" y="253">0.08</text>
            </g>
            <g class="w w56">
              <rect x="588" y="156" width="70" height="36"></rect>
              <text x="623" y="179">0.14</text>
            </g>
            <g class="w w56">
              <rect x="588" y="198" width="70" height="36"></rect>
              <text x="623" y="217">0.15</text>
            </g>

            <!-- Nodes -->
            <g class="node i">
              <circle cx="120" cy="120" r="45"></circle>
              <text x="120" y="120">i‚ÇÅ</text>
            </g>
            <g class="node i">
              <circle cx="120" cy="260" r="45"></circle>
              <text x="120" y="260">i‚ÇÇ</text>
            </g>
            <g class="node h">
              <circle cx="440" cy="120" r="45"></circle>
              <text x="440" y="120">h‚ÇÅ</text>
            </g>
            <g class="node h">
              <circle cx="440" cy="240" r="45"></circle>
              <text x="440" y="240">h‚ÇÇ</text>
            </g>
            <g class="node o">
              <circle cx="760" cy="200" r="45"></circle>
              <text x="760" y="200">out</text>
            </g>

            <text class="pred" x="815" y="150" text-anchor="middle">
              prediction
            </text>
            <!-- Arrow from center-bottom of 'prediction' text to edge of output node -->
            <path
              d="M 815 158 L 791 180"
              stroke="#777"
              stroke-width="2.5"
              fill="none"
              marker-end="url(#arrow)"
            />
          </svg>
        </figure>

        <p>
          <strong>Dataset</strong> We have our neural network with initial
          weights. I want it to learn something, how does neural network learn?
          Data !!! For demonstration, let's say we have a dataset with two
          inputs and one output. The dataset is as follows:
          <figure style="margin: 1rem 0">
            <svg
              viewBox="0 0 300 100"
              width="60%"
              role="img"
              aria-label="Two input values and actual output"
            >
              <style>
                .label {
                  font: 600 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #777;
                }
                .circle-text {
                  font: 700 14px system-ui, -apple-system, Segoe UI, Roboto,
                    Helvetica, Arial;
                  fill: #fff;
                  text-anchor: middle;
                  dominant-baseline: middle;
                }
                .divider {
                  stroke: #ccc;
                  stroke-dasharray: 4 4;
                  stroke-width: 1.5;
                }
              </style>

              <!-- Labels -->
              <text class="label" x="60" y="22" text-anchor="middle">
                Input
              </text>
              <text class="label" x="220" y="22" text-anchor="middle">
                Actual output
              </text>

              <!-- Input circles (extra spacing, still centered) -->
              <circle cx="35" cy="60" r="20" fill="#ffb400" />
              <text class="circle-text" x="35" y="60">2</text>

              <circle cx="85" cy="60" r="20" fill="#ffb400" />
              <text class="circle-text" x="85" y="60">3</text>

              <!-- Divider -->
              <line x1="130" y1="15" x2="130" y2="85" class="divider" />

              <!-- Actual output circle -->
              <circle cx="220" cy="60" r="20" fill="#7b3fc1" />
              <text class="circle-text" x="220" y="60">1</text>
            </svg>
          </figure>
        </p>

        <strong>Forward Pass</strong>
      </section>

      <section id="activation-functions">
        <h2>3. Activation Functions</h2>

        <blockquote>
          Activation functions introduce non-linearity into neural networks.
          Without them, no matter how many layers we stack, the network would
          only be capable of learning linear relationships - severely limiting
          its power.
        </blockquote>

        <p>Common activation functions include:</p>
        <ul>
          <li>
            <strong>ReLU (Rectified Linear Unit):</strong> $f(x) = \max(0, x)$ -
            Simple, effective, most popular
          </li>
          <li>
            <strong>Sigmoid:</strong> $f(x) = \frac{1}{1 + e^{-x}}$ - Outputs
            between 0 and 1
          </li>
          <li>
            <strong>Tanh:</strong> $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ -
            Outputs between -1 and 1
          </li>
          <li>
            <strong>Softmax:</strong> Converts outputs to probabilities for
            multi-class classification
          </li>
        </ul>
        <p>
          ReLU's popularity stems from its simplicity and effectiveness at
          mitigating the vanishing gradient problem that plagued earlier
          activation functions.
        </p>
      </section>

      <section id="network-architectures">
        <h2>4. Popular Network Architectures</h2>

        <blockquote>
          Different types of neural networks excel at different tasks.
          Convolutional Neural Networks (CNNs) dominate computer vision,
          Recurrent Neural Networks (RNNs) excel at sequential data, and
          Transformers have revolutionized natural language processing.
        </blockquote>

        <p><strong>Convolutional Neural Networks (CNNs):</strong></p>
        <p>
          CNNs use convolution operations to detect features like edges,
          textures, and patterns in images. They're designed to be
          translation-invariant, meaning they can recognize a cat whether it
          appears in the top-left or bottom-right of an image.
        </p>

        <p><strong>Recurrent Neural Networks (RNNs):</strong></p>
        <p>
          RNNs maintain internal memory to process sequential data like text or
          time series. Modern variants like LSTMs and GRUs solve the vanishing
          gradient problem that limited earlier RNN architectures.
        </p>

        <p><strong>Transformer Networks:</strong></p>
        <p>
          Transformers use attention mechanisms to process sequences in parallel
          rather than sequentially. This architecture powers modern language
          models like GPT and has achieved state-of-the-art results across many
          domains.
        </p>
      </section>

      <section id="common-challenges">
        <h2>5. Common Challenges and Solutions</h2>

        <blockquote>
          Deep learning practitioners face several recurring challenges:
          overfitting (memorizing training data), vanishing gradients (weights
          in early layers barely update), and finding the right hyperparameters.
        </blockquote>

        <p><strong>Overfitting:</strong></p>
        <ul>
          <li>Use dropout during training to randomly ignore some neurons</li>
          <li>
            Apply regularization techniques (L1, L2) to penalize large weights
          </li>
          <li>Collect more diverse training data</li>
          <li>Use data augmentation to artificially increase dataset size</li>
        </ul>

        <p><strong>Vanishing Gradients:</strong></p>
        <ul>
          <li>Use ReLU activation functions instead of sigmoid/tanh</li>
          <li>Apply batch normalization to stabilize training</li>
          <li>Implement residual connections (skip connections)</li>
          <li>Use proper weight initialization schemes</li>
        </ul>

        <p><strong>Hyperparameter Tuning:</strong></p>
        <ul>
          <li>Start with proven architectures and modify gradually</li>
          <li>Use learning rate scheduling</li>
          <li>Employ validation sets to guide parameter choices</li>
          <li>Consider automated hyperparameter optimization tools</li>
        </ul>
      </section>

      <section id="practical-applications">
        <h2>6. Real-World Applications</h2>

        <blockquote>
          Deep learning has achieved remarkable success across diverse fields:
          image recognition surpassing human performance, natural language
          processing enabling sophisticated AI assistants, and game-playing AI
          mastering complex strategic games.
        </blockquote>

        <p><strong>Computer Vision:</strong></p>
        <ul>
          <li>Medical image analysis for disease diagnosis</li>
          <li>Autonomous vehicle perception systems</li>
          <li>Facial recognition and object detection</li>
          <li>Image generation and style transfer</li>
        </ul>

        <p><strong>Natural Language Processing:</strong></p>
        <ul>
          <li>Machine translation between languages</li>
          <li>Sentiment analysis and text classification</li>
          <li>Chatbots and conversational AI</li>
          <li>Text generation and summarization</li>
        </ul>

        <p><strong>Other Domains:</strong></p>
        <ul>
          <li>Drug discovery and molecular design</li>
          <li>Financial fraud detection</li>
          <li>Recommendation systems</li>
          <li>Climate modeling and weather prediction</li>
        </ul>
      </section>

      <section id="getting-started">
        <h2>7. Getting Started</h2>

        <blockquote>
          The best way to learn deep learning is through hands-on practice.
          Start with established frameworks like TensorFlow or PyTorch, work
          through tutorials with real datasets, and gradually tackle more
          complex projects.
        </blockquote>

        <p><strong>Learning Path:</strong></p>
        <ol>
          <li>
            <strong>Mathematics Foundation:</strong> Linear algebra, calculus,
            probability
          </li>
          <li><strong>Programming Skills:</strong> Python, NumPy, pandas</li>
          <li>
            <strong>Deep Learning Frameworks:</strong> TensorFlow/Keras or
            PyTorch
          </li>
          <li>
            <strong>Practice Projects:</strong> Image classification, sentiment
            analysis
          </li>
          <li>
            <strong>Advanced Topics:</strong> Specialized architectures,
            optimization
          </li>
        </ol>

        <p><strong>Practical Tips:</strong></p>
        <ul>
          <li>
            Start simple - don't jump into complex architectures immediately
          </li>
          <li>Focus on data quality over quantity</li>
          <li>Use pre-trained models when possible (transfer learning)</li>
          <li>Keep detailed logs of experiments and results</li>
          <li>Join communities and read recent research papers</li>
        </ul>
      </section>

      <section id="future-directions">
        <h2>8. Future Directions</h2>

        <blockquote>
          Deep learning continues to evolve rapidly. Current research focuses on
          making models more efficient, interpretable, and capable of learning
          from fewer examples. Emerging areas include few-shot learning, neural
          architecture search, and integration with symbolic reasoning.
        </blockquote>

        <p>
          The field is moving toward more efficient and generalizable AI
          systems. Key trends include developing smaller models that perform as
          well as larger ones, creating systems that can explain their
          decisions, and building AI that can learn new tasks quickly with
          minimal data - much like humans do.
        </p>
        <p>
          As we advance, the integration of deep learning with other AI
          approaches promises even more powerful and versatile systems, bringing
          us closer to artificial general intelligence while addressing current
          limitations in robustness, interpretability, and efficiency.
        </p>
      </section>

      <section id="comments">
        <script
          src="https://utteranc.es/client.js"
          repo="COD1995/ml-meta"
          issue-term="pathname"
          label="comment"
          theme="github-light"
          crossorigin="anonymous"
          async
        ></script>
      </section>

      <footer>
        <p>
          ¬© 2025 Bob Guo ‚Ä¢
          <a
            href="https://github.com/COD1995/ml-meta"
            target="_blank"
            rel="noopener"
          >
            View on GitHub
          </a>
        </p>
      </footer>
    </div>
    <script type="module" src="../../assets/js/main.js"></script>
    <script type="module" src="../../assets/js/chapter-page.js"></script>

    <!-- 1Ô∏è‚É£ auto‚Äëgenerated by the build script (local to this folder) -->
    <script src="book-data.js"></script>

    <!-- 2Ô∏è‚É£ builds the sidebar & mini‚ÄëTOC in the browser -->
    <script type="module" src="../../assets/js/build-side-nav.js"></script>
  </body>
</html>
