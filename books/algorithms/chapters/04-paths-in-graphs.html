
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 4 ‚Äî Paths in Graphs</title>

    <link rel="stylesheet" href="../../../assets/css/base.css" />
    <link rel="stylesheet" href="../../../assets/css/inline-styles.css" />
    <link rel="stylesheet" href="../../../assets/css/chapters.css" />

    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css"
    />

    <script src="../../../assets/js/mathjax-config.js"></script>

    <script
      defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"
    ></script>
    <script
      defer
      src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"
    ></script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.8.0/dist/highlightjs-line-numbers.min.js"
    ></script>
  </head>

  <body>
    <div id="reading-progress">
      <div id="reading-progress-bar"></div>
    </div>

    <nav class="side-nav">
      <div class="side-nav-controls">
        <button
          id="homeBtn"
          class="btn-toggle"
          aria-label="Home"
          onclick="location.href='../../../index.html'"
        >
          üè†
        </button>
        <span class="nav-divider"></span>
        <button
          id="themeToggle"
          class="btn-toggle"
          aria-label="Toggle dark mode"
        >
          üåì
        </button>
      </div>

      <hr class="side-nav-divider" />

      <div id="bookNav"></div>

      <hr class="side-nav-divider" />

      <div class="coffee-banner">
        <a
          href="https://buymeacoffee.com/guoj1995"
          id="coffeeButton"
          target="_blank"
          rel="noopener noreferrer"
          aria-label="Support ML-Meta on Buy Me a Coffee"
        >
          <svg
            width="24"
            height="24"
            viewBox="0 0 24 24"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
          >
            <path d="M20 3H4V5H20V3Z" fill="currentColor" />
            <path
              d="M20 7H4C2.9 7 2 7.9 2 9V17C2 18.1 2.9 19 4 19H10C11.1 19 12 18.1 12 17V16H16C18.2 16 20 14.2 20 12V9C20 7.9 19.1 7 18 7H20Z"
              fill="currentColor"
            />
          </svg>
          <span>Support Us</span>
        </a>
      </div>

      <div class="slack-banner">
        <a
          href="https://join.slack.com/t/mlmetacommunity/shared_invite/zt-38mj0hx5v-8GyxvZ7lanC9HbywfUOwJw"
          id="slackButton"
          target="_blank"
          rel="noopener"
          aria-label="Join our Slack Community"
        >
          <svg
            xmlns="http://www.w3.org/2000/svg"
            viewBox="0 0 122.8 122.8"
            fill="#fff"
            style="width: 1.5em; height: 1.5em; vertical-align: middle"
          >
            <path d="M30.3 78.6c0 5-4 9-9 9s-9-4-9-9 4-9 9-9h9v9z" />
            <path
              d="M34.8 78.6c0-5 4-9 9-9s9 4 9 9v22.5c0 5-4 9-9 9s-9-4-9-9V78.6z"
            />
            <path d="M44 30.3c-5 0-9-4-9-9s4-9 9-9 9 4 9 9v9H44z" />
            <path
              d="M44 34.8c5 0 9 4 9 9s-4 9-9 9H21.5c-5 0-9-4-9-9s4-9 9-9H44z"
            />
            <path d="M92.5 44c0-5 4-9 9-9s9 4 9 9-4 9-9 9h-9V44z" />
            <path
              d="M88 44c0 5-4 9-9 9s-9-4-9-9V21.5c0-5 4-9 9-9s9 4 9 9V44z"
            />
            <path d="M78.8 92.5c5 0 9 4 9 9s-4 9-9 9-9-4-9-9v-9h9z" />
            <path
              d="M78.8 88c-5 0-9-4-9-9s4-9 9-9h22.5c5 0 9 4 9 9s-4 9-9 9H78.8z"
            />
          </svg>
          <span style="margin-left: 0.5em; vertical-align: middle"
            >Join our Slack</span
          >
        </a>
      </div>
    </nav>
    <div class="container content">
      <h1>Chapter 4: Paths in Graphs</h1>

      <section id="distances">
        <h2>4.1 Distances</h2>
        <p>
          Depth-first search readily identifies all the vertices of a graph that
          can be reached from a designated starting point. It also finds
          explicit paths to these vertices, summarized in its search tree
          (Figure 4.1). However, these paths might not be the most economical
          ones possible. In the figure, vertex C is reachable from S by
          traversing just one edge, while the DFS tree shows a path of length 3.
          This chapter is about algorithms for finding shortest paths in graphs.
        </p>
        <p>
          Path lengths allow us to talk quantitatively about the extent to which
          different vertices of a graph are separated from each other:
        </p>
        <blockquote>
          The distance between two nodes is the length of the shortest path
          between them.
        </blockquote>
        <p>
          To get a concrete feel for this notion, consider a physical
          realization of a graph that has a ball for each vertex and a piece of
          string for each edge. If you lift the ball for vertex s high enough,
          the other balls that get pulled up along with it are precisely the
          vertices reachable from s. And to find their distances from s, you
          need only measure how far below s they hang.
        </p>

        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-1.png"
            alt="A simple graph and its depth-first search tree."
            width="700"
          />
          <figcaption>
            Figure 4.1 (a) A simple graph and (b) its depth-first search tree.
          </figcaption>
        </figure>

        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-2.png"
            alt="A physical model of a graph."
            width="700"
          />
          <figcaption>Figure 4.2 A physical model of a graph.</figcaption>
        </figure>
        <p>
          In Figure 4.2 for example, vertex B is at distance 2 from S, and there
          are two shortest paths to it. When S is held up, the strings along
          each of these paths become taut. On the other hand, edge (D, E) plays
          no role in any shortest path and therefore remains slack.
        </p>
      </section>

      <section id="bfs">
        <h2>4.2 Breadth-first search</h2>
        <p>
          In Figure 4.2, the lifting of s partitions the graph into layers: s
          itself, the nodes at distance 1 from it, the nodes at distance 2 from
          it, and so on. A convenient way to compute distances from s to the
          other vertices is to proceed layer by layer. Once we have picked out
          the nodes at distance 0, 1, 2, ..., d, the ones at d + 1 are easily
          determined: they are precisely the as-yet-unseen nodes that are
          adjacent to the layer at distance d. This suggests an iterative
          algorithm in which two layers are active at any given time: some layer
          d, which has been fully identified, and d + 1, which is being
          discovered by scanning the neighbors of layer d.
        </p>
        <p>
          Breadth-first search (BFS) directly implements this simple reasoning
          (Figure 4.3). Initially the queue Q consists only of s, the one node
          at distance 0. And for each subsequent distance d = 1, 2, 3, ...,
          there is a point in time at which Q contains all the nodes at distance
          d and nothing else. As these nodes are processed (ejected off the
          front of the queue), their as-yet-unseen neighbors are injected into
          the end of the queue.
        </p>
        <pre class="pseudocode">
            \begin{algorithm}
            \caption{Breadth-first search.}
            \begin{algorithmic}
              \Procedure{bfs}{$G, s$}
                \ForAll{$u \in V$}
                  \State $dist(u) \gets \infty$
                \EndFor
                \State $dist(s) \gets 0$
                \State $Q \gets [s]$ \Comment{Queue containing just s}
                \While{$Q$ is not empty}
                  \State $u \gets \text{eject}(Q)$
                  \ForAll{edges $(u, v) \in E$}
                    \If{$dist(v) = \infty$}
                      \State $\text{inject}(Q, v)$
                      \State $dist(v) \gets dist(u) + 1$
                    \EndIf
                  \EndFor
                \EndWhile
              \EndProcedure
            \end{algorithmic}
            \end{algorithm}
          </pre>
        <p>
          Let's try out this algorithm on our earlier example (Figure 4.1) to
          confirm that it does the right thing. If S is the starting point and
          the nodes are ordered alphabetically, they get visited in the
          sequence shown in Figure 4.4. The breadth-first search tree, on the
          right, contains the edges through which each node is initially
          discovered. Unlike the DFS tree we saw earlier, it has the property
          that all its paths from S are the shortest possible. It is therefore a
          shortest-path tree.
        </p>

        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-4.png"
            alt="The result of breadth-first search on the graph of Figure 4.1."
            width="700"
          />
          <figcaption>
            Figure 4.4 The result of breadth-first search on the graph of Figure
            4.1.
          </figcaption>
        </figure>

        <h3>Correctness and efficiency</h3>
        <p>
          We have developed the basic intuition behind breadth-first search. In
          order to check that the algorithm works correctly, we need to make
          sure that it faithfully executes this intuition. What we expect,
          precisely, is that
        </p>
        <blockquote>
          For each $d = 0, 1, 2, \dots$, there is a moment at which (1) all
          nodes at distance $\le d$ from s have their distances correctly set;
          (2) all other nodes have their distances set to $\infty$; and (3) the
          queue contains exactly the nodes at distance $d$.
        </blockquote>
        <p>
          This has been phrased with an inductive argument in mind. We have
          already discussed both the base case and the inductive step.
        </p>
        <p>
          The overall running time of this algorithm is linear,
          $O(|V| + |E|)$, for exactly the same reasons as depth-first search.
          Each vertex is put on the queue exactly once, when it is first
          encountered, so there are $2|V|$ queue operations. The rest of the
          work is done in the algorithm's innermost loop. Over the course of
          execution, this loop looks at each edge once (in directed graphs) or
          twice (in undirected graphs), and therefore takes $O(|E|)$ time.
        </p>
        <p>
          Now that we have both BFS and DFS before us: how do their exploration
          styles compare? Depth-first search makes deep incursions into a graph,
          retreating only when it runs out of new nodes to visit. This strategy
          gives it the wonderful, subtle, and extremely useful properties we saw
          in Chapter 3. But it also means that DFS can end up taking a long and
          convoluted route to a vertex that is actually very close by, as in
          Figure 4.1. Breadth-first search makes sure to visit vertices in
          increasing order of their distance from the starting point. This is a
          broader, shallower search, rather like the propagation of a wave upon
          water. And it is achieved using almost exactly the same code as DFS,
          but with a queue in place of a stack.
        </p>
        <p>
          Also notice one stylistic difference from DFS: since we are only
          interested in distances from s, we do not restart the search in other
          connected components. Nodes not reachable from s are simply ignored.
        </p>
      </section>

      <section id="edge-lengths">
        <h2>4.3 Lengths on edges</h2>
        <p>
          Breadth-first search treats all edges as having the same length. This
          is rarely true in applications where shortest paths are to be found.
          For instance, suppose you are driving from San Francisco to Las Vegas,
          and want to find the quickest route. Figure 4.5 shows the major
          highways you might conceivably use. Picking the right combination of
          them is a shortest-path problem in which the length of each edge (each
          stretch of highway) is important. For the remainder of this chapter,
          we will deal with this more general scenario, annotating every edge
          $e \in E$ with a length $l_e$. If $e = (u, v)$, we will sometimes also
          write $l(u,v)$ or $l_{uv}$.
        </p>
        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-5.png"
            alt="A map of cities and highways with distances, illustrating that edge lengths often matter."
            width="700"
          />
          <figcaption>Figure 4.5 Edge lengths often matter.</figcaption>
        </figure>
        <p>
          These $l_e$'s do not have to correspond to physical lengths. They
          could denote time (driving time between cities) or money (cost of
          taking a bus), or any other quantity that we would like to conserve.
          In fact, there are cases in which we need to use negative lengths, but
          we will briefly overlook this particular complication.
        </p>
      </section>

      <section id="dijkstras-algorithm">
        <h2>4.4 Dijkstra's algorithm</h2>
        <h3>4.4.1 An adaptation of breadth-first search</h3>
        <p>
          Breadth-first search finds shortest paths in any graph whose edges
          have unit length. Can we adapt it to a more general graph $G = (V,E)$
          whose edge lengths $l_e$ are positive integers?
        </p>
        <h4>A more convenient graph</h4>
        <p>
          Here is a simple trick for converting G into something BFS can handle:
          break G's long edges into unit-length pieces, by introducing "dummy"
          nodes. Figure 4.6 shows an example of this transformation. To
          construct the new graph G', for any edge $e = (u, v)$ of E, replace
          it by $l_e$ edges of length 1, by adding $l_e - 1$ dummy nodes between
          u and v.
        </p>
        <p>
          Graph G' contains all the vertices V that interest us, and the
          distances between them are exactly the same as in G. Most
          importantly, the edges of G' all have unit length. Therefore, we can
          compute distances in G by running BFS on G'.
        </p>

        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-6.png"
            alt="Breaking edges into unit-length pieces by adding dummy nodes."
            width="700"
          />
          <figcaption>
            Figure 4.6 Breaking edges into unit-length pieces.
          </figcaption>
        </figure>

        <h4>Alarm clocks</h4>
        <p>
          If efficiency were not an issue, we could stop here. But when G has
          very long edges, the G' it engenders is thickly populated with dummy
          nodes, and the BFS spends most of its time diligently computing
          distances to these nodes that we don't care about at all.
        </p>
        <p>
          To see this more concretely, consider the graphs G and G' of Figure
          4.7, and imagine that the BFS, started at node s of G', advances by
          one unit of distance per minute. For the first 99 minutes it
          tediously progresses along S-A and S-B, an endless desert of dummy
          nodes. Is there some way we can snooze through these boring phases and
          have an alarm wake us up whenever something interesting is happening,
          specifically, whenever one of the real nodes (from the original graph
          G) is reached?
        </p>
        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-7.png"
            alt="BFS on a graph with dummy nodes can be uneventful for long periods."
            width="700"
          />
          <figcaption>
            Figure 4.7 BFS on G' is mostly uneventful. The dotted lines show
            some early "wavefronts."
          </figcaption>
        </figure>
        <p>
          We do this by setting two alarms at the outset, one for node A, set to
          go off at time T = 100, and one for B, at time T = 200. These are
          estimated times of arrival, based upon the edges currently being
          traversed. We doze off and awake at T = 100 to find A has been
          discovered. At this point, the estimated time of arrival for B is
          adjusted to T = 150 and we change its alarm accordingly.
        </p>
        <p>
          More generally, at any given moment the breadth-first search is
          advancing along certain edges of G, and there is an alarm for every
          endpoint node toward which it is moving, set to go off at the
          estimated time of arrival at that node. Some of these might be
          overestimates because BFS may later find shortcuts, as a result of
          future arrivals elsewhere. In the preceding example, a quicker route
          to B was revealed upon arrival at A. However, nothing interesting can
          possibly happen before an alarm goes off. The sounding of the next
-         alarm must therefore signal the arrival of the wavefront to a real
-         node $u \in V$ by BFS. At that point, BFS might also start advancing
-         along some new edges out of u, and alarms need to be set for their
-         endpoints.
        </p>
        <p>
          The "alarm clock algorithm" faithfully simulates the execution of BFS
          on G'.
        </p>
        <ul>
          <li>Set an alarm clock for node s at time 0.</li>
          <li>
            Repeat until there are no more alarms: Say the next alarm goes off
            at time T, for node u. Then:
            <ul>
              <li>The distance from s to u is T.</li>
              <li>
                For each neighbor v of u in G:
                <ul>
                  <li>
                    If there is no alarm yet for v, set one for time
                    $T + l(u, v)$.
                  </li>
                  <li>
                    If v's alarm is set for later than $T + l(u, v)$, then reset
                    it to this earlier time.
                  </li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
        <p>
          <b>Dijkstra's algorithm.</b> The alarm clock algorithm computes
          distances in any graph with positive integral edge lengths. It is
          almost ready for use, except that we need to somehow implement the
          system of alarms. The right data structure for this job is a
          <strong>priority queue</strong> (usually implemented via a heap), which maintains a set
          of elements (nodes) with associated numeric key values (alarm times)
          and supports the following operations:
        </p>
        <ul>
            <li><b>Insert.</b> Add a new element to the set.</li>
            <li><b>Decrease-key.</b> Accommodate the decrease in key value of a particular element.</li>
            <li><b>Delete-min.</b> Return the element with the smallest key, and remove it from the set.</li>
            <li><b>Make-queue.</b> Build a priority queue out of the given elements, with the given key values.</li>
        </ul>
        <p>
            The first two let us set alarms, and the third tells us which alarm is next to go off. Putting
            this all together, we get Dijkstra's algorithm (Figure 4.8).
        </p>
        <pre class="pseudocode">
            \begin{algorithm}
            \caption{Dijkstra's shortest-path algorithm.}
            \begin{algorithmic}
              \Procedure{dijkstra}{$G, l, s$}
                \ForAll{$u \in V$}
                  \State $dist(u) \gets \infty$
                  \State $prev(u) \gets \text{nil}$
                \EndFor
                \State $dist(s) \gets 0$
                \State $H \gets \text{makequeue}(V)$ \Comment{using dist-values as keys}
                \While{$H$ is not empty}
                  \State $u \gets \text{deletemin}(H)$
                  \ForAll{edges $(u, v) \in E$}
                    \If{$dist(v) > dist(u) + l(u, v)$}
                      \State $dist(v) \gets dist(u) + l(u, v)$
                      \State $prev(v) \gets u$
                      \State $\text{decreasekey}(H, v)$
                    \EndIf
                  \EndFor
                \EndWhile
              \EndProcedure
            \end{algorithmic}
            \end{algorithm}
          </pre>
          <p>In the code, dist(u) refers to the current alarm clock setting for node u. A value of $\infty$ means the alarm hasn't so far been set. There is also a special array, prev, that holds on one crucial piece of information for each node u: the identity of the node immediately before it on the shortest path from s to u. By following these back-pointers, we can easily reconstruct shortest paths, and so this array is a compact summary of all the paths found. A full example of the algorithm's operation, along with the final shortest-path tree, is shown in Figure 4.9.</p>

        <figure>
          <img
            src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-9.png"
            alt="A complete run of Dijkstra's algorithm."
            width="700"
          />
          <figcaption>
            Figure 4.9 A complete run of Dijkstra's algorithm, with node A as
            the starting point. Also shown are the associated dist values and
            the final shortest-path tree.
          </figcaption>
        </figure>
        <p>In summary, we can think of Dijkstra's algorithm as just BFS, except it uses a priority queue instead of a regular queue, so as to prioritize nodes in a way that takes edge lengths into account. This viewpoint gives a concrete appreciation of how and why the algorithm works, but there is a more direct, more abstract derivation that doesn't depend upon BFS at all. We now start from scratch with this complementary interpretation.</p>

        <h3>4.4.2 An alternative derivation</h3>
        <p>
          Here's a plan for computing shortest paths: expand outward from the
          starting point s, steadily growing the region of the graph to which
          distances and shortest paths are known. This growth should be
          orderly, first incorporating the closest nodes and then moving on to
          those further away. More precisely, when the "known region" is some
          subset of vertices R that includes s, the next addition to it should
          be the node outside R that is closest to s. Let us call this node v;
          the question is: how do we identify it?
        </p>
        <p>
            Since we are assuming that all edge lengths are positive, u must be closer to s than v is. This means that u is in R, otherwise it would contradict v's status as the closest node to s outside R. So, the shortest path from s to v is simply a known shortest path extended by a single edge.
        </p>
        <p>
            But there will typically be many single-edge extensions of the currently known shortest paths (Figure 4.10); which of these identifies v? The answer is, the shortest of these extended paths. Because, if an even shorter single-edge-extended path existed, this would once more contradict v's status as the node outside R closest to s. So, it's easy to find v: it is the node outside R for which the smallest value of $distance(s, u) + l(u, v)$ is attained, as u ranges over R. In other words, try all single-edge extensions of the currently known shortest paths, find the shortest such extended path, and proclaim its endpoint to be the next node of R.
        </p>
        <figure>
            <img
              src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-10.png"
              alt="Single-edge extensions of known shortest paths."
              width="700"
            />
            <figcaption>
              Figure 4.10 Single-edge extensions of known shortest paths.
            </figcaption>
          </figure>
        <p>
            We now have an algorithm for growing R by looking at extensions of the current set of shortest paths. Some extra efficiency comes from noticing that on any given iteration, the only new extensions are those involving the node most recently added to region R. All other extensions will have been assessed previously and do not need to be recomputed.
        </p>

        <h3>4.4.3 Running time</h3>
        <p>At the level of abstraction of Figure 4.8, Dijkstra's algorithm is structurally identical to breadth-first search. However, it is slower because the priority queue primitives are computationally more demanding than the constant-time `eject`'s and `inject`'s of BFS. Since `makequeue` takes at most as long as $|V|$ insert operations, we get a total of $|V|$ `deletemin` and $|V| + |E|$ `insert`/`decreasekey` operations. The time needed for these varies by implementation; for instance, a binary heap gives an overall running time of $O((|V| + |E|) \log |V|)$.</p>
        
        <h4>Which heap is best?</h4>
        <p>The running time of Dijkstra's algorithm depends heavily on the priority queue implementation used. Here are the typical choices.</p>
        <table>
            <thead>
                <tr>
                    <th>Implementation</th>
                    <th>deletemin</th>
                    <th>insert/decreasekey</th>
                    <th>$|V| \cdot \text{deletemin} + (|V| + |E|) \cdot \text{insert}$</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Array</td>
                    <td>$O(|V|)$</td>
                    <td>$O(1)$</td>
                    <td>$O(|V|^2)$</td>
                </tr>
                <tr>
                    <td>Binary heap</td>
                    <td>$O(\log |V|)$</td>
                    <td>$O(\log |V|)$</td>
                    <td>$O((|V| + |E|) \log |V|)$</td>
                </tr>
                <tr>
                    <td>d-ary heap</td>
                    <td>$O(d \log_d |V|)$</td>
                    <td>$O(\log_d |V|)$</td>
                    <td>$O((|V| \cdot d + |E|) \log_d |V|)$</td>
                </tr>
                <tr>
                    <td>Fibonacci heap</td>
                    <td>$O(\log |V|)$ (amortized)</td>
                    <td>$O(1)$ (amortized)</td>
                    <td>$O(|V| \log |V| + |E|)$</td>
                </tr>
            </tbody>
        </table>

      </section>

      <section id="priority-queues">
        <h2>4.5 Priority queue implementations</h2>
        <h3>4.5.1 Array</h3>
        <p>
            The simplest implementation of a priority queue is as an unordered array of key values for all potential elements (the vertices of the graph, in the case of Dijkstra's algorithm). Initially, these values are set to $\infty$.
        </p>
        <p>
            An <code>insert</code> or <code>decreasekey</code> is fast, because it just involves adjusting a key value, an $O(1)$ operation. To <code>deletemin</code>, on the other hand, requires a linear-time scan of the list.
        </p>
        <h3>4.5.2 Binary heap</h3>
        <p>
            Here elements are stored in a complete binary tree, namely, a binary tree in which each level is filled in from left to right, and must be full before the next level is started. In addition, a special ordering constraint is enforced: the key value of any node of the tree is less than or equal to that of its children. In particular, therefore, the root always contains the smallest element.
        </p>
        <figure>
            <img
              src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-11.png"
              alt="Operations on a binary heap."
              width="700"
            />
            <figcaption>
              Figure 4.11 (a) A binary heap with 10 elements. (b)-(d) The "bubble-up" steps in inserting an element. (e)-(g) The "sift-down" steps in a delete-min operation.
            </figcaption>
          </figure>
        <p>
            To <code>insert</code>, place the new element at the bottom of the tree (in the first available position), and let it "bubble up." That is, if it is smaller than its parent, swap the two and repeat. The number of swaps is at most the height of the tree, which is $\lfloor\log_2 n\rfloor$ when there are $n$ elements. A <code>decreasekey</code> is similar, except that the element is already in the tree, so we let it bubble up from its current position.
        </p>
        <p>
            To <code>deletemin</code>, return the root value. To then remove this element from the heap, take the last node in the tree and place it at the root. Let it "sift down": if it is bigger than either child, swap it with the smaller child and repeat. Again this takes $O(\log n)$ time.
        </p>

        <h3>4.5.3 d-ary heap</h3>
        <p>
            A d-ary heap is identical to a binary heap, except that nodes have $d$ children instead of just two. This reduces the height of a tree with $n$ elements to $\Theta(\log_d n) = \Theta((\log n)/(\log d))$. Inserts are therefore speeded up by a factor of $\Theta(\log d)$. Deletemin operations, however, take a little longer, namely $O(d \log_d n)$.
        </p>
      </section>

      <section id="negative-edges">
        <h2>4.6 Shortest paths in the presence of negative edges</h2>
        <h3>4.6.1 Negative edges</h3>
        <p>
            Dijkstra's algorithm works in part because the shortest path from the starting point s to any node v must pass exclusively through nodes that are closer than v. This no longer holds when edge lengths can be negative. In Figure 4.12, the shortest path from S to A passes through B, a node that is further away!
        </p>
        <figure>
            <img
              src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-12.png"
              alt="Graph with a negative edge where Dijkstra's algorithm fails."
              width="500"
            />
            <figcaption>
              Figure 4.12 Dijkstra's algorithm will not work if there are negative edges.
            </figcaption>
          </figure>
        <p>What needs to be changed in order to accommodate this new complication? A crucial invariant in Dijkstra's algorithm is that the <code>dist</code> values it maintains are always either overestimates or exactly correct. They start off at $\infty$, and the only way they ever change is by updating along an edge:</p>
        <pre><code class="language-generic">procedure update((u, v) in E)
    dist(v) = min(dist(v), dist(u) + l(u, v))</code></pre>
        <p>This update operation is simply an expression of the fact that the distance to v cannot possibly be more than the distance to u, plus $l(u, v)$. It has the following properties.</p>
        <ol>
            <li>It gives the correct distance to v in the particular case where u is the second-last node in the shortest path to v, and dist(u) is correctly set.</li>
            <li>It will never make dist(v) too small, and in this sense it is safe. For instance, a slew of extraneous updates can't hurt.</li>
        </ol>
        <p>
            This operation is extremely useful: it is harmless, and if used carefully, will correctly set distances. In fact, Dijkstra's algorithm can be thought of simply as a sequence of updates. We know this particular sequence doesn't work with negative edges, but is there some other sequence that does? A shortest path can have at most $|V|-1$ edges. If the sequence of updates performed includes $(s, u_1), (u_1, u_2), \dots, (u_k, t)$ in that order, then the distance to t will be correctly computed.
        </p>
        <p>
            If we don't know all the shortest paths beforehand, how can we be sure to update the right edges in the right order? Here is an easy solution: simply update all the edges, $|V|-1$ times! The resulting $O(|V| \cdot |E|)$ procedure is called the Bellman-Ford algorithm and is shown in Figure 4.13, with an example run in Figure 4.14.
        </p>
        <pre class="pseudocode">
            \begin{algorithm}
            \caption{The Bellman-Ford algorithm.}
            \begin{algorithmic}
              \Procedure{shortest-paths}{$G, l, s$}
                \ForAll{$u \in V$}
                  \State $dist(u) \gets \infty$
                  \State $prev(u) \gets \text{nil}$
                \EndFor
                \State $dist(s) \gets 0$
                \Repeat{$|V|-1$ times}
                  \ForAll{edge $e=(u,v) \in E$}
                    \State $update(e)$ \Comment{dist(v) = min(dist(v), dist(u) + l(u,v))}
                  \EndFor
                \EndRepeat
              \EndProcedure
            \end{algorithmic}
            \end{algorithm}
          </pre>

          <figure>
            <img
              src="../../../assets/images/algorithms/chpt_4_imgs/figure-4-14.png"
              alt="The Bellman-Ford algorithm illustrated on a sample graph."
              width="700"
            />
            <figcaption>
              Figure 4.14 The Bellman-Ford algorithm illustrated on a sample graph.
            </figcaption>
          </figure>

        <h3>4.6.2 Negative cycles</h3>
        <p>
            If the length of an edge in the graph created a negative cycle, it wouldn't make sense to ask about shortest paths. We could go around the cycle multiple times and find paths of arbitrarily small length.
        </p>
        <p>
            The shortest-path problem is ill-posed in graphs with negative cycles. Our algorithm from Section 4.6.1 works only in the absence of such cycles. Fortunately, it is easy to automatically detect negative cycles and issue a warning. Such a cycle would allow us to endlessly apply rounds of update operations, reducing dist estimates every time. So instead of stopping after $|V|-1$ iterations, perform one extra round. There is a negative cycle if and only if some dist value is reduced during this final round.
        </p>
      </section>

      <section id="dags">
        <h2>4.7 Shortest paths in dags</h2>
        <p>
            There are two subclasses of graphs that automatically exclude the possibility of negative cycles: graphs without negative edges, and graphs without cycles. We already know how to efficiently handle the former. We will now see how the single-source shortest-path problem can be solved in just linear time on directed acyclic graphs (DAGs).
        </p>
        <p>
            In any path of a dag, the vertices appear in increasing linearized order. Therefore, it is enough to linearize (that is, topologically sort) the dag by depth-first search, and then visit the vertices in sorted order, updating the edges out of each. The algorithm is given in Figure 4.15.
        </p>
        <pre class="pseudocode">
            \begin{algorithm}
            \caption{A single-source shortest-path algorithm for dags.}
            \begin{algorithmic}
              \Procedure{dag-shortest-paths}{$G, l, s$}
                \ForAll{$u \in V$}
                  \State $dist(u) \gets \infty$
                  \State $prev(u) \gets \text{nil}$
                \EndFor
                \State $dist(s) \gets 0$
                \State Linearize G
                \ForAll{each $u \in V$, in linearized order}
                  \ForAll{edges $(u, v) \in E$}
                    \State $update((u,v))$
                  \EndFor
                \EndFor
              \EndProcedure
            \end{algorithmic}
            \end{algorithm}
          </pre>
        <p>Notice that our scheme doesn't require edges to be positive. In particular, we can find longest paths in a dag by the same algorithm: just negate all edge lengths.</p>
      </section>

      <section id="comments">
        <script
          src="https://utteranc.es/client.js"
          repo="COD1995/ml-meta"
          issue-term="pathname"
          label="comment"
          theme="github-light"
          crossorigin="anonymous"
          async
        ></script>
      </section>

      <footer>
        <p>
          ¬© 2025 Bob Guo ‚Ä¢
          <a
            href="https://github.com/COD1995/ml-meta"
            target="_blank"
            rel="noopener"
          >
            View on GitHub
          </a>
        </p>
      </footer>
    </div>

    <script type="module" src="../../../assets/js/main.js"></script>
    <script type="module" src="../../../assets/js/chapter-page.js"></script>

    <script src="book-data.js"></script>

    <script type="module" src="../../../assets/js/build-side-nav.js"></script>
  </body>
</html>
```