<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Chapter 4: Paths in Graphs</title>

    <link rel="stylesheet" href="../../../assets/css/base.css" />
    <link rel="stylesheet" href="../../../assets/css/chapters.css" />

    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css"
    />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

    <script>
      window.MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"],
          ],
        },
      };
    </script>
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>

    <script
      defer
      src="https://cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js"
    ></script>
  </head>

  <body>
    <div class="layout">
      <aside id="page-sidebar" class="sidebar">
        <div class="sidebar-header">
          <a class="home-link" href="../../../index.html">üè†¬†Home¬†Page</a>
          <button
            id="sidebarToggle"
            class="sidebar-toggle"
            aria-label="Toggle sidebar"
            aria-expanded="true"
          >
            &#9776;
          </button>
        </div>

        <div class="sidebar-inner">
          <nav>
            <h3>
              <a class="section-link" href="../index.html">üìñ¬†Algorithms</a>
            </h3>
            <ul class="book-nav">
              <li class="chapter-details">
                <details open>
                  <summary>Chapter¬†4:¬†Paths¬†in¬†Graphs</summary>
                  <ul id="toc-ch04" class="toc-list" data-src="self"></ul>
                </details>
              </li>
              <li class="chapter-details">
                <details>
                  <summary>Chapter¬†5:¬†Greedy Algorithms</summary>
                  <ul
                    id="toc-ch05"
                    class="toc-list"
                    data-src="05-greedy-algorithms.html"
                  ></ul>
                </details>
              </li>
              <li class="chapter-details">
                <details>
                  <summary>Chapter¬†6:¬†Dynamic¬†Programming</summary>
                  <ul
                    id="toc-ch06"
                    class="toc-list"
                    data-src="06-dynamic-programming.html"
                  ></ul>
                </details>
              </li>
              <li class="chapter-details">
                <details>
                  <summary>Chapter¬†7:¬†Linear Programming & Reductions</summary>
                  <ul
                    id="toc-ch07"
                    class="toc-list"
                    data-src="07-linear-programming.html"
                  ></ul>
                </details>
              </li>
            </ul>
          </nav>
        </div>
      </aside>

      <main class="content card">
        <h1>Chapter 4: Paths in Graphs</h1>

        <section id="s1">
          <h2>4.1 Distances</h2>
          <p>
            This section introduces the fundamental problem of finding the
            shortest paths in a graph. While Depth-First Search (DFS) can find
            paths from a starting node to all reachable vertices, these paths
            are not guaranteed to be the shortest. For example, in Figure 4.1,
            DFS might find a path from S to C of length 3, even though a direct
            edge exists with length 1. This motivates the need for specialized
            algorithms to find paths with the minimum possible length.
          </p>
          <p>
            The concept of **distance** is formally defined as the length of the
            shortest path between two nodes. A helpful physical analogy is to
            imagine a graph as a set of balls (vertices) connected by strings of
            unit length (edges). If you lift the ball for a starting vertex `s`,
            all other balls that get pulled up are the vertices reachable from
            `s`. The vertical distance they hang below `s` corresponds exactly
            to their shortest path distance from `s`. As shown in Figure 4.2,
            this model illustrates how shortest paths become taut, while longer
            paths remain slack.
          </p>
        </section>

        <section id="s2">
          <h2>4.2 Breadth-first search</h2>
          <p>
            Breadth-First Search (BFS) is an algorithm that systematically
            computes distances from a source node by exploring the graph in
            layers. The core idea is to first identify all nodes at distance 1,
            then all nodes at distance 2, and so on. Once the nodes at distance
            `d` are known, the nodes at distance `d+1` are simply their
            unvisited neighbors.
          </p>
          <p>
            The BFS algorithm, shown in Figure 4.3, implements this
            layer-by-layer strategy using a queue (a first-in, first-out data
            structure). The process starts by placing the source vertex `s` (at
            distance 0) into the queue. The algorithm then repeatedly extracts a
            vertex `u` from the front of the queue and adds its unvisited
            neighbors to the back, setting their distance to `dist(u) + 1`. At
            any point, the queue contains all nodes of a specific distance `d`,
            and as they are processed, the nodes for distance `d+1` are queued
            up.
          </p>
          <p>
            When run on the graph in Figure 4.1, BFS explores nodes in
            increasing order of their distance from the source `S`, as traced in
            Figure 4.4. The edges through which nodes are first discovered form
            a **shortest-path tree**, where every path from the root `S` is a
            shortest path in the original graph.
          </p>

          <h3>Correctness and efficiency</h3>
          <p>
            The correctness of BFS is established with an inductive proof based
            on the following invariant: For each distance `d = 0, 1, 2, ...`,
            there's a point in time when the queue `Q` contains exactly the set
            of nodes at distance `d` from the source `s`. At this moment, all
            nodes at distances less than or equal to `d` have their correct
            distances computed, while all others have a distance of infinity.
          </p>
          <p>
            The algorithm's running time is linear, $O(|V| + |E|)$. Each vertex
            is enqueued and dequeued exactly once, leading to $O(|V|)$ time for
            queue operations. The inner loop examines each edge once in a
            directed graph or twice in an undirected graph, resulting in
            $O(|E|)$ time for edge processing.
          </p>
          <p>
            BFS contrasts with DFS in its exploration style. While DFS performs
            deep, narrow searches using a stack, BFS conducts a broad, shallow
            search using a queue, guaranteeing that it finds the shortest path
            in terms of the number of edges. Unlike a full DFS run, BFS
            initiated from a source `s` only visits nodes within the same
            connected component and ignores unreachable vertices.
          </p>
        </section>

        <section id="s3">
          <h2>4.3 Lengths on edges</h2>
          <p>
            While BFS is perfect for graphs where all edges have the same
            length, most real-world applications involve edges with varying
            lengths or weights. For instance, in a road network like the one in
            Figure 4.5, the length of an edge could represent distance, travel
            time, or cost. These weights, denoted as $l_e$, are crucial for
            finding the most economical path. The algorithms discussed in the
            remainder of this chapter address this more general scenario where
            edges have positive lengths, with a later look at the complications
            introduced by negative lengths.
          </p>
        </section>

        <section id="s4">
          <h2>4.4 Dijkstra's algorithm</h2>

          <h3>4.4.1 An adaptation of breadth-first search</h3>
          <p>
            To adapt BFS for graphs with positive integer edge lengths, one
            could transform the graph `G` into an unweighted graph `G'` by
            breaking down each edge of length `l_e` into `l_e` unit-length edges
            using `l_e - 1` dummy nodes (as shown in Figure 4.6). Running BFS on
            this larger graph `G'` would yield correct shortest paths for the
            original graph `G`. However, this approach is inefficient if edge
            lengths are large, as BFS would spend most of its time processing
            dummy nodes.
          </p>
          <p>
            A more efficient method is to simulate this process without creating
            the dummy nodes explicitly. This can be conceptualized as an "alarm
            clock algorithm". Initially, an alarm is set for the source node `s`
            at time 0. When an alarm goes off for a node `u` at time `T`, we
            know `dist(s, u) = T`. We then set or update alarms for all of `u`'s
            neighbors `v` to go off at time `T + l(u, v)`, but only if this new
            time is earlier than `v`'s currently set alarm.
          </p>
          <p>
            This system of alarms is implemented using a **priority queue**, a
            data structure that efficiently manages a set of elements with
            numeric keys (alarm times) and supports `insert`, `decrease-key`,
            and `delete-min` operations. The `delete-min` operation tells us
            which alarm is next to go off. This adaptation results in
            **Dijkstra's algorithm** (Figure 4.8). The algorithm maintains an
            array `dist(u)` for the current shortest distance estimate (alarm
            time) for each node `u` and a `prev(u)` array to reconstruct the
            shortest paths. A full example is shown in Figure 4.9.
          </p>

          <h3>4.4.2 An alternative derivation</h3>
          <p>
            Dijkstra's algorithm can also be derived from a different
            perspective. The algorithm works by growing a region `R` of vertices
            for which the shortest path from `s` is known. Initially, `R`
            contains only `s`. In each step, the algorithm adds the node `v`
            outside of `R` that is closest to `s`.
          </p>
          <p>
            The shortest path from `s` to this node `v` must come from extending
            a shortest path to some node `u` already in `R` by a single edge
            `(u, v)`. This is because all intermediate nodes on a shortest path
            must be closer to `s` than the destination (assuming positive edge
            weights). Therefore, to find `v`, we simply need to check all
            single-edge extensions from the known region `R` and select the one
            that results in the shortest overall path. This greedy approach,
            when implemented with a priority queue to efficiently find the
            minimum distance extension, is identical to Dijkstra's algorithm.
            The correctness can be formally proven by induction on the size of
            the known region `R`.
          </p>

          <h3>4.4.3 Running time</h3>
          <p>
            The running time of Dijkstra's algorithm depends on the priority
            queue implementation. The algorithm performs $|V|$ `delete-min`
            operations and up to $|E|$ `decrease-key` operations.
          </p>
          <ul>
            <li>
              With a simple **array**, `delete-min` takes $O(|V|)$ time and
              `decrease-key` takes $O(1)$, for a total time of $O(|V|^2)$. This
              is efficient for dense graphs where $|E|$ is close to $|V|^2$.
            </li>
            <li>
              With a **binary heap**, both operations take $O(\log|V|)$ time,
              resulting in a total of $O((|V| + |E|)\log|V|)$. This is
              preferable for sparse graphs.
            </li>
            <li>
              A **d-ary heap** can be optimized for the graph's density by
              setting `d` ‚âà $|E|/|V|$, achieving nearly linear time for graphs
              of intermediate density.
            </li>
            <li>
              A **Fibonacci heap** provides the best asymptotic time of
              $O(|V|\log|V| + |E|)$ by making `decrease-key` an $O(1)$ amortized
              operation, though it is more complex to implement.
            </li>
          </ul>
        </section>

        <section id="s5">
          <h2>4.5 Priority queue implementations</h2>
          <p>
            This section details the data structures commonly used for priority
            queues in Dijkstra's algorithm.
          </p>

          <h3>4.5.1 Array</h3>
          <p>
            The most basic implementation uses an unordered array. While
            `insert` and `decrease-key` are constant-time operations ($O(1)$),
            finding the minimum element (`delete-min`) requires a linear scan of
            the entire array, which takes $O(|V|)$ time.
          </p>

          <h3>4.5.2 Binary heap</h3>
          <p>
            A more efficient implementation uses a **binary heap**, which is a
            complete binary tree where every node's key is smaller than or equal
            to its children's keys. This ensures the root always holds the
            minimum element. Operations like `insert` ("bubble-up") and
            `delete-min` ("sift-down") both take $O(\log n)$ time, where `n` is
            the number of elements in the heap. The heap's regular structure
            allows it to be easily stored in an array.
          </p>

          <h3>4.5.3 d-ary heap</h3>
          <p>
            A **d-ary heap** generalizes the binary heap by giving each node `d`
            children instead of two. This reduces the tree's height to $O(\log_d
            n)$, speeding up `insert` operations. However, `delete-min` becomes
            slightly slower at $O(d \log_d n)$ because a node must be compared
            with all `d` of its children during the "sift-down" process.
          </p>
        </section>

        <section id="s6">
          <h2>4.6 Shortest paths in the presence of negative edges</h2>

          <h3>4.6.1 Negative edges</h3>
          <p>
            Dijkstra's algorithm fails if a graph contains negative edge
            weights. The core assumption that a shortest path to a vertex `v`
            must pass only through vertices closer than `v` is violated in this
            case. As shown in Figure 4.12, the shortest path from S to A is
            S‚ÜíB‚ÜíA, which passes through B, a node that is farther away.
          </p>
          <p>
            To handle negative edges, a different approach is needed. The
            fundamental operation for finding shortest paths is the `update`
            rule: `dist(v) = min(dist(v), dist(u) + l(u,v))`. This operation is
            always safe (it never makes a distance estimate incorrect) and is
            effective if applied correctly. A shortest path from `s` to `t` can
            have at most $|V|-1$ edges. If we apply the `update` operation for
            every edge on this path in sequence, we will find the correct
            shortest distance.
          </p>
          <p>
            Since we don't know the shortest paths in advance, a simple and
            robust solution is to just relax every edge in the graph $|V|-1$
            times. This guarantees that all shortest path distances are
            correctly computed. This procedure is known as the **Bellman-Ford
            algorithm** (Figure 4.13), and it runs in $O(|V| \cdot |E|)$ time.
            An optimization is to stop early if a full pass over all edges
            produces no updates. Figure 4.14 shows an example run.
          </p>

          <h3>4.6.2 Negative cycles</h3>
          <p>
            The concept of a "shortest path" is ill-defined in graphs that
            contain a **negative cycle**, a cycle whose edges sum to a negative
            value. By repeatedly traversing such a cycle, one can find paths of
            arbitrarily small length. The Bellman-Ford algorithm assumes no
            negative cycles exist.
          </p>
          <p>
            However, Bellman-Ford can be used to detect negative cycles. If,
            after $|V|-1$ rounds of updates, an additional round still manages
            to decrease any distance estimate, it indicates the presence of a
            negative cycle.
          </p>
        </section>

        <section id="s7">
          <h2>4.7 Shortest paths in dags</h2>
          <p>
            Directed acyclic graphs (DAGs) are a special class of graphs that
            cannot have cycles, and therefore cannot have negative cycles. This
            structural property allows for a highly efficient, linear-time
            algorithm for finding shortest paths.
          </p>
          <p>
            The key insight is that in any path within a DAG, the vertices
            appear in an order consistent with a topological sort
            (linearization) of the graph. Therefore, if we process the vertices
            in a topologically sorted order, we can guarantee that by the time
            we compute `dist(v)`, the distance values for all its predecessors
            `u` have already been finalized. The algorithm (Figure 4.15) first
            linearizes the DAG and then iterates through the vertices in this
            order, applying the `update` rule to all outgoing edges. This
            algorithm works correctly even with negative edge weights and can
            also be adapted to find the longest paths by simply negating all
            edge lengths.
          </p>
        </section>

        <section id="comments">
          <script
            src="[https://utteranc.es/client.js](https://utteranc.es/client.js)"
            repo="YOUR_GITHUB_REPO"
            issue-term="pathname"
            label="comment"
            theme="github-light"
            crossorigin="anonymous"
            async
          ></script>
        </section>

        <section id="comments">
          <script
            src="https://utteranc.es/client.js"
            repo="COD1995/ml-meta"
            issue-term="pathname"
            label="comment"
            theme="github-light"
            crossorigin="anonymous"
            async
          ></script>
        </section>

        <footer>
          <p>
            ¬© 2025 Kristopher Kodweis ‚Ä¢
            <a href="YOUR_GITHUB_LINK" target="_blank" rel="noopener">
              View on GitHub
            </a>
          </p>
        </footer>
      </main>
    </div>

    <script type="module" src="../../../assets/js/main.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/lib/highlight.min.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/highlightjs-line-numbers.js@2.8.0/dist/highlightjs-line-numbers.min.js"></script>

    <script>
      document.addEventListener("DOMContentLoaded", () => {
        /* 1. Render Pseudocode blocks */
        pseudocode.renderClass("pseudocode", {
          lineNumber: true,
          lineNumberPunc: ".",
          indentSize: "1.4em",
        });

        /* 2. Render MathJax */
        window.MathJax?.typeset();

        /* 3. Activate Highlight.js */
        hljs.highlightAll();
        hljs.initLineNumbersOnLoad(); // ‚úÖ Add this line
      });
    </script>
  </body>
</html>
